{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a4e964-c967-4500-8fe5-0223da4a35af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lnxin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.848 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "jieba.load_userdict('customdict.txt')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "plt.rcParams['font.family'] = 'simhei'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d48434-dd2f-4d2e-9f6e-edc3c5887789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 基础功能函数\n",
    "def clean_text(text):\n",
    "    # 只保留汉字与标点符号。\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fff。，？、；：（）「」《》“”‘’——……]', '', text)\n",
    "    return text\n",
    "\n",
    "def open_file(fname):\n",
    "    # 接受文件名，返回清洗后的字符串\n",
    "    with open(fname, \"r\",encoding = \"utf-8\", errors = \"ignore\") as f:\n",
    "        text=f.read()\n",
    "    return clean_text(text)\n",
    "\n",
    "def write_file(content:list, fname):\n",
    "    # 将列表内容写入文件\n",
    "    with open(fname, 'w', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        # 列表每个元素占一行\n",
    "        for word, freq in tqdm(content):\n",
    "            f.write(f\"'{word}', {freq}\\n\")\n",
    "\n",
    "def load_stopwords(fname):\n",
    "    stopwords = set()\n",
    "    with open(fname, 'r', encoding = \"utf-8\", errors = \"ignore\") as f:\n",
    "        stopwords.update(line.strip() for line in f if line.strip())\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def seg(text):\n",
    "    # 分词，返回词语列表\n",
    "    words = pseg.cut(text)\n",
    "    # 过滤掉标点（非语素:x）\n",
    "    wordlist = [w for w,t in words if t != 'x']\n",
    "    return wordlist\n",
    "\n",
    "\n",
    "def posseg(text):\n",
    "    # 带词性标注分词，返回词性列表\n",
    "    words = pseg.cut(text)\n",
    "    # 过滤掉标点（非语素:x）\n",
    "    taglist = [t for w,t in words if t != 'x']\n",
    "    return taglist\n",
    "\n",
    "\n",
    "def filtered_seg(text):\n",
    "    # 返回词语+词类的元组列表\n",
    "    stopwords = load_stopwords('hit_stopwords.txt')\n",
    "    words = pseg.cut(text)\n",
    "    return [(w, t) for w, t in words if t != 'x' and w not in stopwords]\n",
    "\n",
    "\n",
    "def count(lst):\n",
    "    # 接收列表，返回降序排列的列表->[('a', 5), ('b', 2), ('r', 2)]\n",
    "    lst = [item for item in lst if item != '' or item !=' ']\n",
    "    count = Counter(lst)\n",
    "    return count.most_common()\n",
    "\n",
    "\n",
    "def bi_gram(lst):\n",
    "    # 二元统计。接受（分词后的）列表，返回二元统计列表 \n",
    "    return [(lst[i],lst[i+1]) for i in range(len(lst)-1)]\n",
    "\n",
    "\n",
    "def tri_gram(lst):\n",
    "    # 三元统计。接受（分词后的）列表，返回三元统计列表\n",
    "    return [(lst[i],lst[i+1],lst[i+2]) for i in range(len(lst)-2)]\n",
    "\n",
    "\n",
    "def plot_top_words(lst, title, xlabel, ylabel, colour, suffix='.png'):\n",
    "    # 接受排序好的以元祖为元素的列表，以条形图的方式可视化，并保存图片\n",
    "    items = [' '.join(x[0]) if isinstance(x[0], tuple) else x[0] for x in lst]\n",
    "    freq = [x[1] for x in lst]\n",
    "    fname = f'{title}{suffix}'\n",
    "    plt.figure(figsize=(10, 6)) \n",
    "    plt.bar(items, freq, align = \"center\", color = colour)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(xlabel,fontsize=12)\n",
    "    plt.ylabel(ylabel,fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=400)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e97756e-564d-4e3a-9d77-9fbc1049ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整合函数功能\n",
    "def analyse_text(text):\n",
    "    # 对一部作品实现六个指标的统计\n",
    "    words_with_tags = filtered_seg(text)\n",
    "    words = [w for w,t in words_with_tags]\n",
    "    tags = [t for w,t in words_with_tags]\n",
    "    return {\n",
    "        '词语词频列表': count(words),\n",
    "        '二元词频率列表': count(bi_gram(words)),\n",
    "        '三元词频率列表': count(tri_gram(words)),\n",
    "        '词类频率列表': count(tags),\n",
    "        '二元词类频率列表': count(bi_gram(tags)),\n",
    "        '三元词类频率列表': count(tri_gram(tags))\n",
    "    }\n",
    "\n",
    "\n",
    "def to_file(result:dict, prefix:str, output_dir: str = '词类词频统计结果'):\n",
    "    # 将analyse_text生成的每项结果写入文件\n",
    "    try: # 确保输出目录存在\n",
    "        os.makedirs(output_dir, exist_ok=True)  # 递归创建目录（如果不存在）\n",
    "    except Exception as e:\n",
    "        print(f'创建输出目录 {output_dir} 时出错：{e}')\n",
    "        return\n",
    "    # 结果写入文件\n",
    "    for name in result:\n",
    "        data = result[name] # 以元组为元素的列表\n",
    "        filename = f'{prefix}_{name}.txt'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                for j,k in data:\n",
    "                    f.write(f'{j},{k}\\n')\n",
    "            print(f'{filename}输出成功')\n",
    "        except Exception as e:\n",
    "            print(f'写入文件 {filename} 时出错：{e}')\n",
    "\n",
    "\n",
    "def plot_all(result, title_prefix):\n",
    "    try:\n",
    "        top_words = result['词语词频列表'][:20]\n",
    "        plot_top_words(top_words, f'{title_prefix}_top20词语','词语','词频',colour='steelblue')\n",
    "        print(f'{title_prefix}_top20词语输出成功')\n",
    "    except Exception as e:\n",
    "        print(f'{title_prefix}:词语词频图失败:{e}')\n",
    "\n",
    "    try:\n",
    "        top_bigram = result['二元词频率列表'][:20]\n",
    "        plot_top_words(top_bigram, f'{title_prefix}_top20二元词语','二元词','词频', colour='mediumseagreen')\n",
    "        print(f'{title_prefix}_top20二元词语输出成功')\n",
    "    except Exception as e:\n",
    "        print(f'{title_prefix}:二元词词频图失败:{e}')\n",
    "\n",
    "    try:\n",
    "        top_trigram = result['三元词频率列表'][:20]\n",
    "        plot_top_words(top_trigram, f'{title_prefix}_top20三元词语','三元词','词频', colour='coral')\n",
    "        print(f'{title_prefix}_top20三元词语输出成功')\n",
    "    except Exception as e:\n",
    "        print(f'{title_prefix}:三元词词频图失败:{e}')\n",
    "\n",
    "    try:\n",
    "        top_tags = result['词类频率列表'][:20]\n",
    "        plot_top_words(top_tags, f'{title_prefix}_top20词类','词类','频率',colour='skyblue')\n",
    "        print(f'{title_prefix}_top20词类输出成功')\n",
    "    except Exception as e:\n",
    "        print(f'{title_prefix}:词类频率图失败:{e}')\n",
    "\n",
    "    try:\n",
    "        top_bitags = result['二元词类频率列表'][:20]\n",
    "        plot_top_words(top_bitags, f'{title_prefix}_top20二元词类','二元词类','频率',colour='mediumaquamarine')\n",
    "        print(f'{title_prefix}_top20二元词类输出成功')\n",
    "    except Exception as e:\n",
    "        print(f'{title_prefix}:二元词类频率图失败:{e}')\n",
    "\n",
    "    try:\n",
    "        top_tritags = result['三元词类频率列表'][:20]\n",
    "        plot_top_words(top_tritags, f'{title_prefix}_top20三元词类','三元词类','频率',colour='lightsalmon')\n",
    "        print(f'{title_prefix}_top20三元词类输出成功')\n",
    "    except Exception as e:\n",
    "        print(f'{title_prefix}:三元词类频率图失败:{e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75b896-46b8-4996-8ebd-0f9ef5bcb73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = {'萧红':['生死场.txt','呼兰河传.txt','马伯乐.txt','小城三月.txt'],\n",
    "           '丁玲':['韦护.txt','一九三零.txt','母亲.txt','桑干河.txt']\n",
    "}\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1. 生成六个指标的文件\n",
    "    for author, works in tqdm(authors.items()):\n",
    "        all_text = ''\n",
    "        \n",
    "        # 以作品为单位进行统计\n",
    "        for filepath in tqdm(works):\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    text = clean_text(f.read())\n",
    "                all_text += text + '\\n'  # 合并所有作品文本\n",
    "\n",
    "                # 统计单个作品、可视化输出\n",
    "                result = analyse_text(text)\n",
    "                fname = f'{author}_{filepath[:-4]}'\n",
    "                plot_all(result, title_prefix=fname)\n",
    "                to_file(result, fname)\n",
    "            except Exception as e:\n",
    "                print(f'处理作品 {filepath} 时出错：{e}')\n",
    "\n",
    "        # 以作家为单位的统计\n",
    "        try: \n",
    "            result_all = analyse_text(all_text)\n",
    "            fname_all = f'{author}_all_works'\n",
    "            plot_all(result_all, title_prefix=fname_all)\n",
    "            to_file(result_all, fname_all)\n",
    "        except Exception as e:\n",
    "            print(f'写入作家{author}的统计文件时出错：{e}')\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkuseg",
   "language": "python",
   "name": "pkuseg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
