{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911c21e8-a8f7-493f-8cd8-74f7f1f72aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lnxin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.174 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "jieba.load_userdict('customdict.txt')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'simhei'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108bbca-eb8f-4c73-ae3f-13d436e21fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础功能函数\n",
    "def is_chinese_char(c):\n",
    "    # 判断字符是否为汉字\n",
    "    return '\\u4e00' <= c <= '\\u9fff'\n",
    "\n",
    "\n",
    "def merge_aa(wordlist):\n",
    "    # 扫描分词列表，将连续两个单字合并\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(wordlist):\n",
    "        if (\n",
    "            i < len(wordlist) - 1\n",
    "            and len(wordlist[i]) == len(wordlist[i+1]) == 1\n",
    "            and wordlist[i] == wordlist[i+1]\n",
    "            and is_chinese_char(wordlist[i])\n",
    "        ):\n",
    "            merged.append(wordlist[i] + wordlist[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            merged.append(wordlist[i])\n",
    "            i += 1\n",
    "    return merged\n",
    "\n",
    "\n",
    "def is_dup(word):\n",
    "    # 接受词语，判断是否为叠词\n",
    "    # 叠词种类：AA, AAB, ABB, AABB, ABAB, ABCC, LONG（多个单字词）\n",
    "    stopwords = ['珊珊','亭亭',]\n",
    "    if any(stop in word for stop in stopwords):  # 过滤掉一些人名（并不属于需要统计的叠词类型）\n",
    "        return None\n",
    "    n = len(word)\n",
    "    aux = {'的', '地', '得', '着', '了'}  # 含一些助词的AA式叠词会被jieba分词为三字词，需要防止误判为AAB\n",
    "    if n==2:\n",
    "        if word[0]==word[1]:\n",
    "            return 'AA'\n",
    "    elif n==3:\n",
    "        if (word[0]==word[1] and word[2] in aux):\n",
    "            return 'AA'\n",
    "        if (word[0]==word[1] and word[1] != word[2]):\n",
    "            return 'AAB'\n",
    "        elif (word[0]!=word[1] and word[1]==word[2]):\n",
    "            return 'ABB'\n",
    "    elif n==4:\n",
    "        if (word[0]==word[1] and word[2]==word[3] and word[1]!=word[2]):\n",
    "            return 'AABB'\n",
    "        elif (word[0]==word[2] and word[1]==word[3] and word[0]!=word[1]):\n",
    "            return 'ABAB'\n",
    "        elif (word[0]!=word[1] and word[2]==word[3] and word[2] != word[0] and word[2] != word[1]):\n",
    "            return 'ABCC'\n",
    "        elif (word[0]==word[1] and word[2]!=word[3] and word[1] != word[2] and word[1] != word[3]):\n",
    "            return 'AABC'\n",
    "    elif n >= 3 and all(c == word[0] for c in word):\n",
    "        return 'LONG'\n",
    "    return None\n",
    "\n",
    "\n",
    "def count_dup(wordlist):\n",
    "    # 统计叠词。接受分词列表，返回叠词类型统计和叠词词典。\n",
    "    word_dict = defaultdict(list)\n",
    "    n = len(wordlist)\n",
    "    # 词内叠词\n",
    "    for i, word in enumerate(wordlist):\n",
    "        result = is_dup(word)\n",
    "        if result:\n",
    "            word_dict[result].append(word)\n",
    "    # 检测ABAB\n",
    "    i=0\n",
    "    while i < n-1:\n",
    "        w1, w2 = wordlist[i], wordlist[i+1]\n",
    "        if (len(w1)==2 and len(w2)==2 and w1==w2):\n",
    "            abab_word = w1+w2\n",
    "            word_dict['ABAB'].append(abab_word)\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "    return dict(word_dict)\n",
    "\n",
    "\n",
    "def pie(data_dict:dict,author,suffix='.png'):\n",
    "    # 接受叠词词典，绘制以作者为单位的叠词分布饼图\n",
    "    color_map = {\n",
    "        'AA': '#ff9999',\n",
    "        'AAB': '#66b3ff',\n",
    "        'ABB': '#99ff99',\n",
    "        'AABB': '#ffcc99',\n",
    "        'ABAB': '#c2c2f0',\n",
    "        'AABC': '#ffb3e6',\n",
    "        'ABCC': '#c2f0c2',\n",
    "    }\n",
    "    labels = [k for k in color_map if k in data_dict]\n",
    "    sizes = [len(data_dict[k]) for k in labels]\n",
    "    colors = [color_map[k] for k in labels]\n",
    "    fname = f'{author}{suffix}'\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.pie(\n",
    "        sizes,\n",
    "        labels=labels,\n",
    "        autopct='%1.1f%%',  \n",
    "        startangle=90,\n",
    "        colors=colors\n",
    "    )\n",
    "    plt.title(f'{author}的叠词分布', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc=\"lower right\",ncol=2)\n",
    "    plt.savefig(fname, dpi=400)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # 只保留汉字与标点符号。\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fff。，？、；：（）「」《》“”‘’——……]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def seg(text):\n",
    "    # 分词，返回词语列表\n",
    "    return jieba.lcut(text)\n",
    "\n",
    "\n",
    "def open_file(fname):\n",
    "    # 接受文件名，返回清洗后的字符串\n",
    "    with open(fname, \"r\",encoding = \"utf-8\", errors = \"ignore\") as f:\n",
    "        text=f.read()\n",
    "    return clean_text(text)\n",
    "\n",
    "\n",
    "def write_file(result:dict, fname):\n",
    "    # 将叠词处理结果写入文件\n",
    "    with open(fname, 'w', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for cat,word in result.items():\n",
    "            words = '、'.join(word)\n",
    "            f.write(f'{cat}类叠词个数{len(word)}：{words}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ce5b0-ed90-4266-959e-4a523c7a803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此指定需要分析的作者及作品\n",
    "authors = {'萧红':['1生死场.txt','2呼兰河传.txt','3马伯乐.txt','4小城三月.txt'],\n",
    "           '丁玲':['1韦护.txt','2一九三零.txt','3母亲.txt','4桑干河.txt']\n",
    "}\n",
    "\n",
    "def main():\n",
    "    for author, files in authors.items():\n",
    "        print(f'现在处理作者：{author}\\n')\n",
    "        author_words = defaultdict(list)\n",
    "        for filename in files:\n",
    "            try:\n",
    "                with open(filename, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                # 进行作品的统计\n",
    "                try:\n",
    "                    wordlist = list(seg(text))\n",
    "                    merged = merge_aa(wordlist)\n",
    "                    word_dict = count_dup(merged)  # word_dict是叠词字典，键为叠词类型\n",
    "                    print(f'作品《{filename[:-4]}》中：')\n",
    "                    for cat,word in word_dict.items():\n",
    "                        print(f'{cat}类叠词个数{len(word)}')\n",
    "                except Exception as e:\n",
    "                    print(f'进行词语处理时出错：{e}')\n",
    "                try:\n",
    "                    write_file(word_dict, f'叠词统计_{filename[:-4]}.txt')\n",
    "                except Exception as e:\n",
    "                    print(f'未能成功写入文件：{e}')\n",
    "\n",
    "                # 汇总到作者，便于后续统计\n",
    "                for cat,word in word_dict.items():\n",
    "                    author_words[cat].extend(word)\n",
    "            except Exception as e:\n",
    "                print(f'处理单个作品时出错：{e}')\n",
    "\n",
    "        # 进行作者的统计\n",
    "        try:\n",
    "            write_file(author_words,f'叠词统计_{author}.txt')\n",
    "            pie(author_words,author=author)\n",
    "            print(f'作者{author}的作品中：')\n",
    "            for cat,word in author_words.items():\n",
    "                print(f'{cat}类叠词个数{len(word)}')\n",
    "        except Exception as e:\n",
    "            print(f'作者统计时出错：{e}')\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkuseg",
   "language": "python",
   "name": "pkuseg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
